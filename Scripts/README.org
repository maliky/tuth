#+TITLE: Seed Data Intake & Wrangling Plan
#+AUTHOR: Codex Agent, Malik Konneh
#+DATE: <2025-11-19>

* Scope
This note inventories every asset under =Seed_data/=, explains how each source maps to Tusis models, and captures the tooling already checked into =Scripts/= for data wrangling (name cleanup, schedule reshaping, etc.). Use it as the working log for staging SmartSchool → Tusis imports and for deciding which raw files matter.

* Action Plan (working)
1. **Stage raw dumps**: keep =Sources/Smart_School= and =Sources/GradPro= unzipped, checksum them, and snapshot absolute paths so downstream notebooks/scripts can mount them predictably.
2. **Profile SmartSchool tables** with =Scripts/parsing_db.org=: load CSV/XLS exports via Pandas, prune unused columns, and emit canonical TSV files (the =Parsed/= outputs referenced inside the script) for the fields we actually migrate.
3. **Normalize manual spreadsheets** (=people.ods=, schedule workbooks, prereq documents) into the import-ready CSV schemas defined in the main repo (see =import_resources= and README quickstart).
4. **Reconcile legacy vs new**: only fall back to GradPro extracts (or the Access .mdb) when SmartSchool lacks the record—flag any conflicting IDs in this log before import.
5. **Feed Django** via =manage.py import_resources= (or the per-model variant) once each dataset validates; keep the generated CSVs beside this README for traceability.

* Data Inventory
** SmartSchool exports (=Seed_data/Sources/Smart_School=)
- =dbexport_backup/= and =my_ss_db_backup250708/dbexport_backup/=: identical csv/xls exports (students, studentcourses, gradessheets, files/filedetail, etc.). These match the tables enumerated in =Scripts/parsing_db.org=. Primary focus tables:
  - *UM_Courses*, *UM_CoursesLevels*, *UM_CoursesSections*: drive `Course`, `CurriculumCourse`, and `Section` imports.
  - *UM_Curriculums*, *UM_CurriculumCourses*: feed curricula + mapping.
  - *UM_StudentsCourses*, *UM_Registrations*, *UM_GradeSheet*: academic history + term registrations.
  - *UM_Staff*, *Users*, *AuditTrail*: staff rosters and login metadata for people import + auditing.
  - *UM_Tuition*, *UM_Fees*, *UM_CollegeFees*: finance scaffolding for Payment/Fee defaults.
- =it_ss_db_backup_250520/TU\ DATABASE\ SCRIPT/tudbscript.sql=: UTF-16 SQL Server script that recreates the =SmartSchool_WVSTU= database (CREATE DATABASE + all tables). Potential conversion path: convert to UTF-8 (e.g., `iconv -f UTF-16LE -t UTF-8`) then run through `sqlserver2pgsql` or `pgloader` to target Postgres if we ever need a full RDBMS migration instead of CSV-level wrangling.
- Helper: =Scripts/parsing_db.org= already opens these exports, defines Pandas helpers (=load_xls=, =peek_in=, etc.), and shows which tables were worth extracting. Re-run it from =Seed_data/Scripts= when SmartSchool delivers a fresh dump.

** GradPro legacy (=Seed_data/Sources/GradPro=)
- =GP_DB250717/= holds hundreds of CSVs grouped by subfolder (Medium/Small). Focus if SmartSchool is missing older alumni: *StudentInfo.csv*, *StudentRecords.csv*, *Courses.csv*, *Classes.csv*, *Sessions.csv*, *Prerequisites.csv*. The Access backup (=grapro-backup-20250717-154832.mdb=) remains as the canonical source; use `mdb-export`/`mdbtools` to regenerate CSVs if needed.
- Treat GradPro as read-only reference—only ingest items that do not exist (or conflict) in SmartSchool.

** Manual/curated sources (=Seed_data/Sources=)
- =people.ods=: curated staff/student roster with cleaned names; good seed for `StudentResource` or manual overrides.
- =schedule_25-26s1_cleaned.xlsx= & =schedule_25-26s2-draft.xlsx=: structured timetables already massaged for AY25-26. Each sheet aligns with the expected columns from `db_seed_example.txt` (course code/no, section, faculty, weekday, start/end, room).
- =sessions_24-25s2.csv= & =sessions_25-26s1.csv=: condensed session exports ready for `SecSessionResource` (`weekday`, `start_time`, `end_time`, `room`, `section_no`).
- =Prerequises/= folder: source-of-truth spreadsheets & docs for prerequisites (CAF/S COAS etc). Convert each workbook sheet into the =academics_curriculum_courses.csv= schema defined in the main README.

** Seed CSV templates
- =curricula.csv= & =sections.csv= showcase the cleaned outputs expected by the Django importers (mirror them when generating new files).
- =db_seed_example.txt= documents the row layout consumed by `run.sh` for ad-hoc imports.

** Helper apps (=Seed_data/Scripts/=)
- =class_schedule/= python package used by both CLI + Flask app for cleaning schedules (merging duplicate instructor names, slotting rooms, etc.).
- =schedule_checker_app/= embedded Flask UI (`online_schedule_checker.py`, static Altair charts) for visually verifying the cleaned schedules. The =processed/= folder is where uploaded spreadsheets land.
- =ss_roster2csv/= CLI (see README) to convert SmartSchool roster exports into normalized CSV with cleaned names/usernames; relies on its own `pyproject`. Use this when SmartSchool dumps differ from the Pandas notebook expectations.
- =parsing_db.org=: literate notebook describing data exploration. Keep extending this file when discovering new useful SmartSchool tables; it is the single best map of table/column semantics.

* Wrangling Strategy
1. **SmartSchool pipeline**
   - Execute =parsing_db.org=, which already loads =scdb.xls= + CSV companions, drops all-null columns, and writes TSVs under =Seed_data/Parsed=. Extend its =RESOURCES_MAP= style lists to emit the nine import-ready CSV schemas (curriculum, course, section, student, faculty, room, semester, session, registration/payment) defined in the project root README.
   - Validate each dataset via `resource.import_data(..., dry_run=True)` inside the command before committing.
2. **Manual sheets**
   - Use =Scripts/class_schedule= utils to collapse instructor names (`helper.py` has split-name logic) and to pivot schedule spreadsheets into the `SectionResource` + `SecSessionResource` columns.
   - Feed prerequisite Excel docs through Pandas, map textual program names to `Curriculum.short_name`, then append to =academics_curriculum_courses.csv=.
3. **GradPro fallback**
   - Only touch the tables flagged above; they share column names (e.g., `StudentInfo.csv` has `StudentID`, `Major`, `Status`). Build a lightweight mapper to translate them into the same canonical CSVs used elsewhere.
4. **Database import**
   - Run, in order: `python manage.py create_states`, `python manage.py load_roles`, optional `python manage.py create_test_users`, then `python manage.py import_resources -f <combined.csv>` for bulk loads or `import_resources_individualy -d <dir>` when staging per-table files.
   - Keep a CHANGELOG entry in this README whenever a new CSV is produced so we can track provenance.
5. **Optional full DB conversion**
   - If direct SQL-level migration becomes necessary, spin up a temporary SQL Server instance, run `sqlcmd -i tudbscript.sql`, take a `.bak`, then use `pgloader` (supports SQL Server source) to produce a Postgres schema. Caveat: requires mapping identity columns and date/time precision; still slower than the CSV route but worth considering if we need transactional history intact.

* Outstanding Tasks / Questions
- SmartSchool zipped exports include =logs/= and =Void/= directories—confirm whether any of those tables (voided files, audit logs) must be preserved for compliance.
- Decide whether to source prerequisites exclusively from the curated spreadsheets or to trust SmartSchool’s =UM_CurriculumCourses= pre-req columns.
- Verify if =people.ods= supersedes SmartSchool user data (esp. name spellings) and document whichever source we decide to treat as canonical.
- GradPro `.mdb`: check if `mdbtools` on Linux can open it losslessly; otherwise note that a Windows VM export may be required.
- For the direct SQL conversion idea, we still need to know whether the `.mdf/.ldf` mentioned in `tudbscript.sql` exist anywhere; currently only the script survives.

* Next Steps Log
- [ ] Extend =parsing_db.org= to emit actual CSVs for UM_* tables → =Seed_data/Parsed/=. Track output names here.
- [ ] Re-run =ss_roster2csv= against the latest SmartSchool roster to have clean names before seeding Django.
- [ ] Snapshot checksums (e.g., `shasum`) for each raw dump to detect upstream changes.
